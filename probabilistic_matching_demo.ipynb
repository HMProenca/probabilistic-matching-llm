{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Probabilistic Record Matching Demo\n",
                "\n",
                "This notebook demonstrates the probabilistic record matching system. We will walk through the code step-by-step, explaining each function as we go and inspecting the results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import random\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from itertools import combinations\n",
                "from data_generator import generate_data\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "pd.set_option('display.width', 1000)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Generate Data\n",
                "We generate a synthetic dataset with 80 unique records and 20 perturbed duplicates.\n",
                "We also introduce **missing data** (None) in some fields to simulate real-world imperfections."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = generate_data()\n",
                "fields = ['name', 'address', 'city', 'date_of_birth']\n",
                "print(f\"Generated {len(df)} records.\")\n",
                "print(\"\\nFirst 5 records (note potential None values):\")\n",
                "display(df[fields].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Compute Similarities\n",
                "The first step is to convert the text data in each field into a numerical representation (embedding) using a language model. We then calculate the cosine similarity between all pairs of records for each field.\n",
                "\n",
                "We use `all-MiniLM-L6-v2`, a lightweight and efficient model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_similarities(df, fields, model_name='all-MiniLM-L6-v2'):\n",
                "    \"\"\"\n",
                "    Computes cosine similarity matrices for each field independently.\n",
                "\n",
                "    Reasoning:\n",
                "    ----------\n",
                "    We use a pre-trained sentence transformer model (`all-MiniLM-L6-v2`) to generate dense vector\n",
                "    embeddings for each text field. This model is chosen for its efficiency and good performance\n",
                "    on semantic similarity tasks.\n",
                "\n",
                "    Missing Data Handling:\n",
                "    ----------------------\n",
                "    If a field is missing (None/NaN) for either record in a pair, we set the similarity score to 0.\n",
                "    \n",
                "    Why?\n",
                "    - **Avoid False Positives**: If we simply filled missing values with an empty string, two records\n",
                "      with missing addresses would have a similarity of 1.0 (perfect match of empty strings).\n",
                "      This would falsely boost the match probability.\n",
                "    - **Conservative Approach**: By setting similarity to 0, we treat missing data as providing\n",
                "      *no evidence* of a match. The model must rely on other present fields (like Name) to\n",
                "      make the decision.\n",
                "    \"\"\"\n",
                "    print(\"Computing embeddings and similarities...\")\n",
                "    model = SentenceTransformer(model_name)\n",
                "    sim_matrices = {}\n",
                "    \n",
                "    for f in fields:\n",
                "        # 1. Identify missing values\n",
                "        missing_mask = df[f].isna() | (df[f] == '')\n",
                "        \n",
                "        # 2. Encode (fill NaNs temporarily for encoding)\n",
                "        embeddings = model.encode(df[f].fillna('').astype(str).tolist())\n",
                "        \n",
                "        # 3. Compute Cosine Similarity\n",
                "        sim_matrix = cosine_similarity(embeddings)\n",
                "        \n",
                "        # 4. Apply Mask: Set similarity to 0 if either record has missing data\n",
                "        # We set rows and columns corresponding to missing data to 0\n",
                "        # Note: This sets (missing, missing) to 0, and (missing, present) to 0.\n",
                "        sim_matrix[missing_mask, :] = 0\n",
                "        sim_matrix[:, missing_mask] = 0\n",
                "        \n",
                "        sim_matrices[f] = sim_matrix\n",
                "        \n",
                "    return sim_matrices\n",
                "\n",
                "sim_matrices = compute_similarities(df, fields)\n",
                "\n",
                "# Inspect results\n",
                "print(f\"\\nComputed similarity matrices for fields: {list(sim_matrices.keys())}\")\n",
                "print(f\"Shape of 'name' similarity matrix: {sim_matrices['name'].shape}\")\n",
                "\n",
                "# Show a sample similarity score\n",
                "print(f\"Similarity between record 0 and 1 for 'name': {sim_matrices['name'][0][1]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train Model\n",
                "Instead of manually tuning weights for each field (e.g., deciding that Name is 2x more important than Date of Birth), we use Logistic Regression to learn these weights from the data.\n",
                "\n",
                "We create a training set by:\n",
                "1.  Finding all **positive pairs** (records that are actually the same person).\n",
                "2.  Sampling **negative pairs** (records that are different people).\n",
                "3.  Using the similarity scores of the fields as features to predict whether a pair is a match."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model(df, fields, sim_matrices):\n",
                "    \"\"\"\n",
                "    Trains a Logistic Regression model to classify pairs of records as matches or non-matches.\n",
                "\n",
                "    Reasoning:\n",
                "    ----------\n",
                "    We use Logistic Regression because it provides a probabilistic output and is highly interpretable.\n",
                "    The coefficients (weights) learned by the model tell us exactly how important each field is for\n",
                "    determining a match.\n",
                "\n",
                "    Impact of Missing Data:\n",
                "    -----------------------\n",
                "    Since we set the similarity of missing fields to 0, the model sees these as \"dissimilar\".\n",
                "    However, because we train on many examples, the model learns to weight the *present* fields appropriately.\n",
                "    If 'Address' is frequently missing but 'Name' is always present and accurate, the model might assign\n",
                "    a higher weight to 'Name' and a slightly lower weight to 'Address' (or keep it high, but relying on\n",
                "    it only when present).\n",
                "    \"\"\"\n",
                "    print(\"Training model...\")\n",
                "    pos_pairs = [(i, j) for i, j in combinations(range(len(df)), 2) \n",
                "                 if df.iloc[i]['original_id'] == df.iloc[j]['original_id']]\n",
                "    \n",
                "    neg_pairs = set()\n",
                "    while len(neg_pairs) < len(pos_pairs) * 5:\n",
                "        i, j = sorted(random.sample(range(len(df)), 2))\n",
                "        if df.iloc[i]['original_id'] != df.iloc[j]['original_id']:\n",
                "            neg_pairs.add((i, j))\n",
                "            \n",
                "    X, y = [], []\n",
                "    for pairs, label in [(pos_pairs, 1), (list(neg_pairs), 0)]:\n",
                "        for i, j in pairs:\n",
                "            X.append([sim_matrices[f][i][j] for f in fields])\n",
                "            y.append(label)\n",
                "            \n",
                "    clf = LogisticRegression(class_weight='balanced').fit(X, y)\n",
                "    return clf\n",
                "\n",
                "clf = train_model(df, fields, sim_matrices)\n",
                "\n",
                "# Inspect learned weights\n",
                "print(\"\\nLearned Feature Weights (Importance):\")\n",
                "weights = dict(zip(fields, clf.coef_[0]))\n",
                "display(pd.DataFrame(list(weights.items()), columns=['Field', 'Weight']).sort_values('Weight', ascending=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Predict Matches\n",
                "Now that we have a trained model, we can use it to score every pair of records in the dataset. If the model predicts a match probability greater than 70%, we consider it a match.\n",
                "\n",
                "**Note**: We filter out pairs where the records are identical in all fields, as these are trivial matches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_matches(df, fields, sim_matrices, clf, threshold=0.7):\n",
                "    \"\"\"\n",
                "    Predicts match probabilities for all pairs of records in the dataset.\n",
                "\n",
                "    Reasoning:\n",
                "    ----------\n",
                "    We apply the trained classifier to feature vectors derived from the similarity matrices.\n",
                "    This combines the individual field similarities into a single probability score.\n",
                "\n",
                "    Decisions & Assumptions:\n",
                "    ------------------------\n",
                "    - **Thresholding**: We use a default threshold of 0.7. Pairs with a probability > 0.7 are\n",
                "      considered matches. This higher threshold reduces false positives at the cost of potentially\n",
                "      missing some true matches (lower recall).\n",
                "    - **Filtering Trivial Matches**: We explicitly skip pairs where all fields are identical.\n",
                "    \"\"\"\n",
                "    print(\"Predicting matches...\")\n",
                "    matches = []\n",
                "    for i, j in combinations(range(len(df)), 2):\n",
                "        # Skip if the records are identical in the fields we care about\n",
                "        if all(df.iloc[i][f] == df.iloc[j][f] for f in fields):\n",
                "            continue\n",
                "\n",
                "        features = [sim_matrices[f][i][j] for f in fields]\n",
                "        prob = clf.predict_proba([features])[0][1]\n",
                "        if prob > threshold:\n",
                "            matches.append({\n",
                "                'index_a': i, 'index_b': j,\n",
                "                'record_a': df.iloc[i]['name'], 'record_b': df.iloc[j]['name'], \n",
                "                'score': prob, 'is_match': df.iloc[i]['original_id'] == df.iloc[j]['original_id']\n",
                "            })\n",
                "    return pd.DataFrame(matches).sort_values('score', ascending=False)\n",
                "\n",
                "matches = predict_matches(df, fields, sim_matrices, clf)\n",
                "\n",
                "# Filter out exact name matches to show only fuzzy matches\n",
                "fuzzy_matches = matches[matches['record_a'] != matches['record_b']]\n",
                "\n",
                "print(f\"\\nFound {len(matches)} total matches.\")\n",
                "print(f\"Found {len(fuzzy_matches)} fuzzy matches (excluding exact name matches).\")\n",
                "print(\"\\nTop 10 Fuzzy Matches:\")\n",
                "display(fuzzy_matches.head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Fields Used for Comparison & Example\n",
                "Finally, we list the fields that were used and show a side-by-side comparison of a detected fuzzy match."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Fields used for matching:\")\n",
                "for f in fields:\n",
                "    print(f\"- {f}\")\n",
                "\n",
                "if len(fuzzy_matches) > 0:\n",
                "    print(\"\\nExample Fuzzy Match Comparison (Top Match):\")\n",
                "    # Get indices of the top fuzzy match\n",
                "    top_match = fuzzy_matches.iloc[0]\n",
                "    idx_a = int(top_match['index_a'])\n",
                "    idx_b = int(top_match['index_b'])\n",
                "    \n",
                "    # Extract data for these two records\n",
                "    rec_a = df.iloc[idx_a][fields]\n",
                "    rec_b = df.iloc[idx_b][fields]\n",
                "    \n",
                "    # Create a comparison table\n",
                "    comparison = pd.DataFrame({'Record A': rec_a, 'Record B': rec_b})\n",
                "    display(comparison)\n",
                "else:\n",
                "    print(\"\\nNo fuzzy matches found to display comparison.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Accuracy & Error Analysis\n",
                "We evaluate the model's performance by comparing predictions against ground truth labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not matches.empty:\n",
                "    # Calculate accuracy metrics\n",
                "    true_positives = len(matches[matches['is_match'] == True])\n",
                "    false_positives = len(matches[matches['is_match'] == False])\n",
                "    \n",
                "    # Count actual positives in the dataset\n",
                "    actual_positives = len([(i, j) for i, j in combinations(range(len(df)), 2) \n",
                "                            if df.iloc[i]['original_id'] == df.iloc[j]['original_id']\n",
                "                            and not all(df.iloc[i][f] == df.iloc[j][f] for f in fields)])\n",
                "    \n",
                "    false_negatives = actual_positives - true_positives\n",
                "    \n",
                "    precision = true_positives / len(matches) if len(matches) > 0 else 0\n",
                "    recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
                "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
                "    \n",
                "    print(\"=\" * 50)\n",
                "    print(\"MATCHING PERFORMANCE METRICS\")\n",
                "    print(\"=\" * 50)\n",
                "    print(f\"\\nTrue Positives (Correct Matches): {true_positives}\")\n",
                "    print(f\"False Positives (Incorrect Matches): {false_positives}\")\n",
                "    print(f\"False Negatives (Missed Matches): {false_negatives}\")\n",
                "    print(f\"\\nPrecision: {precision:.2%} (of predicted matches, how many are correct)\")\n",
                "    print(f\"Recall: {recall:.2%} (of actual matches, how many did we find)\")\n",
                "    print(f\"F1 Score: {f1_score:.2%} (harmonic mean of precision and recall)\")\n",
                "    \n",
                "    # Show examples of errors if they exist\n",
                "    if false_positives > 0:\n",
                "        print(\"\\n\" + \"=\" * 50)\n",
                "        print(\"FALSE POSITIVES (Incorrectly Predicted as Matches)\")\n",
                "        print(\"=\" * 50)\n",
                "        false_pos = matches[matches['is_match'] == False].head(5)\n",
                "        display(false_pos[['record_a', 'record_b', 'score']])\n",
                "    \n",
                "    if false_negatives > 0:\n",
                "        print(\"\\n\" + \"=\" * 50)\n",
                "        print(f\"FALSE NEGATIVES: {false_negatives} actual matches were missed\")\n",
                "        print(\"These are pairs with the same original_id but score < 0.7\")\n",
                "        print(\"=\" * 50)\n",
                "else:\n",
                "    print(\"No matches found to evaluate.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Match Probability Statistics\n",
                "We analyze the distribution of match probabilities to understand the model's confidence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not matches.empty:\n",
                "    scores = matches['score']\n",
                "    print(\"Match Probability Statistics:\")\n",
                "    print(f\"Min: {scores.min():.4f}\")\n",
                "    print(f\"Max: {scores.max():.4f}\")\n",
                "    print(f\"Mean: {scores.mean():.4f}\")\n",
                "    print(f\"Median: {scores.median():.4f}\")\n",
                "    print(f\"Std Dev: {scores.std():.4f}\")\n",
                "    \n",
                "    plt.figure(figsize=(10, 6))\n",
                "    plt.hist(scores, bins=20, color='skyblue', edgecolor='black')\n",
                "    plt.title('Distribution of Match Probabilities')\n",
                "    plt.xlabel('Match Probability')\n",
                "    plt.ylabel('Frequency')\n",
                "    plt.grid(axis='y', alpha=0.75)\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No matches found to analyze.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}